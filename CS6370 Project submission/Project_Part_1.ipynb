{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0ccc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for the first exercise of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7200b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "from numpy import log10\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TreebankWordTokenizer, PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7920488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted Index: \n",
      "[('carnivor', [2]), ('deer', [3]), ('eat', [3]), ('eater', [1, 2]), ('grass', [3]), ('herbivor', [1]), ('leav', [3]), ('meat', [1, 2]), ('plant', [1, 2]), ('typic', [1, 2])]\n"
     ]
    }
   ],
   "source": [
    "'''Creating inverted index for a document'''\n",
    "\n",
    "d1 = ['Herbivores are typically plant eaters and not meat eaters . ']\n",
    "d2 = ['Carnivores are typically meat eaters and not plant eaters']\n",
    "d3 = ['Deers eat grass and leaves']\n",
    "\n",
    "stop_words = ['are', 'and', 'not']\n",
    "\n",
    "docs = [d1, d2, d3]\n",
    "\n",
    "def invert_index(docs = docs):\n",
    "    dct = {}; TF = {}; C = []\n",
    "    for i in range(len(docs)):\n",
    "        docs[i] = PunktSentenceTokenizer().tokenize(docs[i][0])    #It takes str as input and outputs list of strings\n",
    "        for sent in docs[i]:\n",
    "            words = TreebankWordTokenizer().tokenize(sent)\n",
    "            \n",
    "        '''Creating inverted index'''\n",
    "        for word in words:\n",
    "            if word.lower() in stop_words or word in string.punctuation:\n",
    "                continue\n",
    "            \n",
    "            word = PorterStemmer().stem(word)\n",
    "            if word not in C:\n",
    "                C.append(word)\n",
    "                dct[word] = [i+1]\n",
    "                TF[word] = [i+1]\n",
    "            else:\n",
    "                if i+1 not in dct[word]:\n",
    "                    dct[word].append(i+1)\n",
    "                '''For creating TF-IDF matrix'''\n",
    "                TF[word].append(i+1)\n",
    "    inverted_index = sorted(dct.items())\n",
    "    return inverted_index, TF, dct\n",
    "\n",
    "inverted_index, _, _ = invert_index()\n",
    "print(f'Inverted Index: \\n{inverted_index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64b0e3f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The term list is as follows  :\n",
      "['carnivor', 'deer', 'eat', 'eater', 'grass', 'herbivor', 'leav', 'meat', 'plant', 'typic']\n",
      "TF-IDF Matrix:\n",
      "TF-IDF for document 1 [0.0, 0.0, 0.0, 0.3521825181113625, 0.0, 0.47712125471966244, 0.0, 0.17609125905568124, 0.17609125905568124, 0.17609125905568124]\n",
      "\n",
      "TF-IDF for document 2 [0.47712125471966244, 0.0, 0.0, 0.3521825181113625, 0.0, 0.0, 0.0, 0.17609125905568124, 0.17609125905568124, 0.17609125905568124]\n",
      "\n",
      "TF-IDF for document 3 [0.0, 0.47712125471966244, 0.47712125471966244, 0.0, 0.47712125471966244, 0.0, 0.47712125471966244, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "'''TF-IDF term document Matrix'''\n",
    "\n",
    "def TF_IDF():\n",
    "    inverted_index, TF, dct = invert_index()\n",
    "    sorted_dict = {k: v for k, v in sorted(dct.items(), key=lambda item: item[0])}\n",
    "    term_list=list(sorted_dict.keys())\n",
    "    print('The term list is as follows  :')\n",
    "    print(term_list)\n",
    "    \n",
    "    TF = sorted(TF.items())\n",
    "    # print('TF', TF)\n",
    "\n",
    "    mat = [[0] * len(TF) for i in range(len(docs))]\n",
    "    df = []\n",
    "    for i in range(len(docs)):            #Iterating over all the words\n",
    "        for j in range(len(TF)):      #Iterating over all the documents\n",
    "            '''Term Frequency'''\n",
    "            mat[i][j] = TF[j][1].count(i+1)\n",
    "            '''TF-IDF without smoothing'''\n",
    "            mat[i][j] = mat[i][j] * log10(len(docs)/len(inverted_index[j][1]))\n",
    "    return mat\n",
    "\n",
    "mat = TF_IDF()\n",
    "\n",
    "#Columns represent words and each row represents a documnent word order is same as Inverted index\n",
    "print('TF-IDF Matrix:')\n",
    "print('TF-IDF for document 1',mat[0])\n",
    "print()\n",
    "print('TF-IDF for document 2',mat[1])\n",
    "print()\n",
    "print('TF-IDF for document 3',mat[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0ae8dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Document retrieval'''\n",
    "def retrieve(query):\n",
    "    _, _, dct = invert_index()\n",
    "    query = TreebankWordTokenizer().tokenize(query)\n",
    "    lst = []\n",
    "    retrived = set()\n",
    "    print('Document retrived for the query:\\n')\n",
    "    for i in range(len(query)):\n",
    "        query[i] = PorterStemmer().stem(query[i])\n",
    "        if query[i] in dct.keys():\n",
    "            dummy = set(dct[query[i]])\n",
    "            lst.append(dummy)\n",
    "            if i == 1: \n",
    "                retrived = lst[1].intersection(lst[0])\n",
    "            elif i>=1: \n",
    "                retrieved.intersection(lst[i])\n",
    "            else: \n",
    "                retrieved = lst[0]\n",
    "    return retrieved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d696af44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document retrived for the query:\n",
      "\n",
      "[1, 2]\n"
     ]
    }
   ],
   "source": [
    "query = 'Plant eaters,'\n",
    "retrived = retrieve(query)\n",
    "print(list(retrived))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8489e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(query):\n",
    "    inverted_index, _, dct = invert_index()\n",
    "    sorted_dict = {k: v for k, v in sorted(dct.items(), key=lambda item: item[0])}\n",
    "    term_list=list(sorted_dict.keys())\n",
    "    query = TreebankWordTokenizer().tokenize(query)\n",
    "    lst = []\n",
    "    retrived = set()\n",
    "    print('Document cosine similarities for the query:\\n')\n",
    "    for i in range(len(query)):\n",
    "        query[i] = PorterStemmer().stem(query[i])\n",
    "    query_vec=np.zeros(len(term_list))\n",
    "    for i in range(len(term_list)):\n",
    "        query_vec[i]=query.count(term_list[i])*(len(mat)/len(inverted_index[i][1]))\n",
    "    doc_sim={}\n",
    "    vector2 = np.array(query_vec)\n",
    "    magnitude2 = np.linalg.norm(vector2)\n",
    "    for i in range(len(mat)):\n",
    "        vector1=np.array(mat[i])\n",
    "        magnitude1=np.linalg.norm(vector1)\n",
    "        dot_product = np.dot(vector1, vector2)\n",
    "        doc_sim[i+1]=dot_product / (magnitude1 * magnitude2)\n",
    "    \n",
    "    return doc_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd582c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document cosine similarities for the query:\n",
      "\n",
      "{1: 0.5601569175157881, 2: 0.5601569175157881, 3: 0.0}\n"
     ]
    }
   ],
   "source": [
    "query = 'Plant eaters,'\n",
    "print(cosine_similarity(query))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
